{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the datasets\n",
    "\n",
    "We will use vanilla PyTorch. We will convert audio to spectrograms and use the Emitter as our label of choice (speaker recognition is an important task, it is assumed that in order for a language to form, one needs to be able to identify who is speaking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.lib.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "import multiprocessing\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = pd.read_csv('data/annotations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep the sample rate of 250_000 Hz but only take the first second of the recording. This can be modified to taking random porition of a file of 1 second duration to improve the results.\n",
    "\n",
    "Additionally, we will zero pad examples that are less than 1 second.\n",
    "\n",
    "Let us load a single wave file to take a closer look at the processing we will apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording, _ = librosa.load(f'data/audio/{anno.iloc[0][\"File Name\"]}', sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336719,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recording.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spec = librosa.feature.melspectrogram(\n",
    "    recording[:250_000], fmin=200, fmax=250_000 // 2, n_mels=64\n",
    ")\n",
    "plt.imshow(spec);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emitter</th>\n",
       "      <th>Addressee</th>\n",
       "      <th>Context</th>\n",
       "      <th>Emitter pre-vocalization action</th>\n",
       "      <th>Addressee pre-vocalization action</th>\n",
       "      <th>Emitter post-vocalization action</th>\n",
       "      <th>Addressee post-vocalization action</th>\n",
       "      <th>File Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emitter  Addressee  Context  Emitter pre-vocalization action  \\\n",
       "0      118          0        9                                2   \n",
       "1        0          0       11                                0   \n",
       "2      118          0       12                                2   \n",
       "3        0          0       12                                0   \n",
       "4        0          0       12                                0   \n",
       "\n",
       "   Addressee pre-vocalization action  Emitter post-vocalization action  \\\n",
       "0                                  2                                 3   \n",
       "1                                  0                                 0   \n",
       "2                                  2                                 3   \n",
       "3                                  0                                 0   \n",
       "4                                  0                                 0   \n",
       "\n",
       "   Addressee post-vocalization action File Name  \n",
       "0                                   3     0.wav  \n",
       "1                                   0     1.wav  \n",
       "2                                   3     2.wav  \n",
       "3                                   0     3.wav  \n",
       "4                                   0     4.wav  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitters = anno.Emitter.unique().tolist()\n",
    "\n",
    "emitter2idx = {emitter: idx for idx, emitter in enumerate(emitters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emitters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0      7858\n",
       "-215    6351\n",
       " 215    6150\n",
       "-231    4303\n",
       "-211    3943\n",
       "        ... \n",
       " 118      62\n",
       "-223      58\n",
       "-108      57\n",
       "-105      39\n",
       "-113       6\n",
       "Name: Emitter, Length: 83, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.Emitter.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is unbalanced to a great extent. This is something to keep in mind when considering the metric to use. Limiting the task to for instance the 20 best represented individuals or addressing the class imbalance via oversampling (or some other method) might also be a viable option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "split = list(skf.split(np.arange(anno.shape[0]), anno.Emitter))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([    1,     2,     3, ..., 91077, 91078, 91079]),\n",
       " array([    0,     8,     9, ..., 91072, 91074, 91076]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0      6287\n",
       "-215    5081\n",
       " 215    4920\n",
       "-231    3442\n",
       "-211    3155\n",
       "        ... \n",
       " 118      49\n",
       "-223      47\n",
       "-108      46\n",
       "-105      31\n",
       "-113       5\n",
       "Name: Emitter, Length: 83, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.iloc[split[0]].Emitter.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0      1571\n",
       "-215    1270\n",
       " 215    1230\n",
       "-231     861\n",
       "-211     788\n",
       "        ... \n",
       " 103      13\n",
       "-108      11\n",
       "-223      11\n",
       "-105       8\n",
       "-113       1\n",
       "Name: Emitter, Length: 83, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.iloc[split[1]].Emitter.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.examples = df\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples.iloc[index]\n",
    "        recording, _ = librosa.load(f'data/audio/{example[\"File Name\"]}', sr=None)\n",
    "        audio = np.zeros((250_000))\n",
    "        audio[:recording.shape[0]] = recording[:250_000]\n",
    "        \n",
    "        x = librosa.feature.melspectrogram(\n",
    "            audio, fmin=200, fmax=250_000, n_mels=64\n",
    "        )\n",
    "        y = emitter2idx[example.Emitter]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.examples.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(anno.iloc[split[0]])\n",
    "valid_ds = Dataset(anno.iloc[split[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    dataset=valid_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl: pass\n",
    "for batch in valid_dl: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 64, 489]), torch.Size([8]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape, batch[1].shape # we are on the final batch, there were not enough examples to fill it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
