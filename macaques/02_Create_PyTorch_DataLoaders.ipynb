{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The audio has been recorded with a sampling rate of 24414 and 44100. We can normalize the recordings to a sample rate of 24414.\n",
    "\n",
    "Also, let's load the data into the pandas dataframe with annotations. The dataset is small and it can fit into our RAM. This means, that every time we iterate over the dataset, we don't have to load every example from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 812 ms, total: 23.5 s\n",
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "anno = pd.read_csv('data/annotations.csv')\n",
    "\n",
    "audio = []\n",
    "\n",
    "for _, row in anno.iterrows():\n",
    "    recording, _ = librosa.load(f'data/{row.split}/{row.filename}', sr=24414)\n",
    "    audio.append(recording)\n",
    "    \n",
    "anno['audio'] = audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only annotations we have available for this dataset are the individual codenames. We will use these as our labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>split</th>\n",
       "      <th>filename</th>\n",
       "      <th>audio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TH</td>\n",
       "      <td>train</td>\n",
       "      <td>TH28.wav</td>\n",
       "      <td>[0.006072998, 0.0051574707, 0.004119873, 0.002...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TH</td>\n",
       "      <td>train</td>\n",
       "      <td>TH22.wav</td>\n",
       "      <td>[0.00894165, 0.009246826, 0.0093688965, 0.0093...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TH</td>\n",
       "      <td>train</td>\n",
       "      <td>TH928.wav</td>\n",
       "      <td>[-0.006164551, -0.0059814453, -0.0056762695, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TH</td>\n",
       "      <td>train</td>\n",
       "      <td>TH1145.wav</td>\n",
       "      <td>[0.005554199, 0.005859375, 0.0061035156, 0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TH</td>\n",
       "      <td>valid</td>\n",
       "      <td>TH470.wav</td>\n",
       "      <td>[-0.006164551, -0.0057678223, -0.005218506, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class  split    filename                                              audio\n",
       "0    TH  train    TH28.wav  [0.006072998, 0.0051574707, 0.004119873, 0.002...\n",
       "1    TH  train    TH22.wav  [0.00894165, 0.009246826, 0.0093688965, 0.0093...\n",
       "2    TH  train   TH928.wav  [-0.006164551, -0.0059814453, -0.0056762695, -...\n",
       "3    TH  train  TH1145.wav  [0.005554199, 0.005859375, 0.0061035156, 0.006...\n",
       "4    TH  valid   TH470.wav  [-0.006164551, -0.0057678223, -0.005218506, -0..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest of recordings is 992 frames. With the sample rate of 24414, this translates to 0.04 seconds of audio.\n",
    "\n",
    "The longest recording is 41307 frames, which translates to 1.69 seconds of audio.\n",
    "\n",
    "Given this, we will provide 3 options for this dataset:\n",
    "* sample random 0.04 seconds from each call for each example (the **sample** option)\n",
    "* cut each example into examples of 0.04 seconds duration (the **cut** option), this will produce some number of new examples, that will depend on the total length of recordings\n",
    "* pad each example to the longest example in the dataset (the **pad** option)\n",
    "* take just the 0.04 of each call from the beginning (the **first** option)\n",
    "\n",
    "Additionally, the classes are unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TH    1345\n",
       "MU    1017\n",
       "IO    1002\n",
       "SN    1001\n",
       "AL     999\n",
       "QU     975\n",
       "BE     478\n",
       "TW     468\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide two versions of the dataset:\n",
    "* **unbalanced** (examples represented in line with counts in the raw dataset)\n",
    "* **balanced** (examples upsampled to count of the most frequently occuring class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleProcessor():\n",
    "    def __init__(self, example_length):\n",
    "        assert example_length in options.keys()\n",
    "        self.example_length = example_length\n",
    "    def __call__(self, example):\n",
    "        return options[self.example_length](example)\n",
    "    \n",
    "    \n",
    "def first(example):\n",
    "    return example[:992]\n",
    "\n",
    "def sample(example):\n",
    "    start_frame = np.random.randint(example.shape[0] - 991)\n",
    "    return example[start_frame:start_frame+992]\n",
    "\n",
    "def pad(example):\n",
    "    out = np.zeros((41307))\n",
    "    out[:example.shape[0]] = example\n",
    "    return out\n",
    "\n",
    "options = {\n",
    "    'first': first,\n",
    "    'sample': sample,\n",
    "    'pad': pad\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a mapping from codename to class idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "codenames = anno['class'].unique().tolist()\n",
    "\n",
    "codename2idx = {codename: idx for idx, codename in enumerate(codenames)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, example='sample', classes='unbalanced'):\n",
    "        if example == 'cut':\n",
    "            cls = []\n",
    "            audio = []\n",
    "\n",
    "            for idx, row in df.iterrows():\n",
    "                while True:\n",
    "                    if row.audio.shape[0] < 992: break\n",
    "                    cls.append(row['class'])\n",
    "                    audio.append(row.audio[:992])\n",
    "                    row.audio = row.audio[992:]\n",
    "            df = pd.DataFrame({'class': cls, 'audio': audio})\n",
    "            example = 'first'\n",
    "            \n",
    "        self.examples = df\n",
    "        self.example_processor = ExampleProcessor(example)\n",
    "        \n",
    "        assert classes in ['balanced', 'unbalanced']\n",
    "        if classes=='balanced':\n",
    "            max_examples = self.examples['class'].value_counts()[0]\n",
    "            for grp in self.examples.groupby('class'):\n",
    "                example_count = self.examples[self.examples['class'] == grp[0]].shape[0]\n",
    "                while example_count < max_examples:\n",
    "                    self.examples = self.examples.append(self.examples[self.examples['class'] == grp[0]][:max_examples-example_count])\n",
    "                    example_count = self.examples[self.examples['class'] == grp[0]].shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        example = self.examples.iloc[index]\n",
    "        x = self.example_processor(example.audio)\n",
    "        y = codename2idx[example['class']]\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.examples.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the components in place, let's work on our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset(anno[anno.split == 'train'], example='cut', classes='balanced')\n",
    "valid_ds = Dataset(anno[anno.split == 'valid'], example='cut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114904"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AL    14363\n",
       "TH    14363\n",
       "IO    14363\n",
       "SN    14363\n",
       "QU    14363\n",
       "TW    14363\n",
       "MU    14363\n",
       "BE    14363\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.examples['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now construct the dataloaders to ensure everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "\n",
    "valid_dl = torch.utils.data.DataLoader(\n",
    "    dataset=valid_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dl: pass\n",
    "for batch in valid_dl: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 992]), torch.Size([24]))"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].shape, batch[1].shape # we are on the final batch, there were not enough examples to fill it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

